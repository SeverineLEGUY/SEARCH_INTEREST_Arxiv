pipeline { //1
    agent any

    environment { //2
        TEST_IMAGE = 'arxive-tests'
        APP_IMAGE = 'arxiv-app'
        // Utiliser withCredentials pour la sécurité et la portée
        // ... (Les variables d'environnement sont bonnes, mais vérifiez les identifiants)
    } //2

    stages { //2
        
        
        stage('Build Test Container') { //3
            steps { //4
                // S'assure que l'agent a le code (via SCM auto-géré) et le construit
                withEnv(["DOCKER_HOST=tcp://jenkins-docker:2376"]) {  //5
                    dir('jenkins') { //6
                    // Le contexte de build est maintenant jenkins/, donc le Dockerfile est relatif à jenkins/
                        sh 'docker build -t arxive-tests -f tests/Dockerfile_test .'
                    } //6 
                 } //5
            } //4
        } //3

        stage('Run Unit Tests') { //3
            steps { //4
                script { //5
                    withEnv(["DOCKER_HOST=tcp://jenkins-docker:2376"]) { //6
                        sh """
                            docker run --rm \\
                                --env AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
                                --env AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
                                --env AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
                                -v ${PWD}/jenkins/tests:/app/tests \\ 
                                ${TEST_IMAGE} \\
                                /bin/bash -c "pytest --junitxml=/app/tests/junit-report.xml"
                        """
                    } //6
                }  //5
            } //4
            post { //4
                always {  //5
                    junit 'jenkins/tests/junit-report.xml' 
                } //5
            } //4
        } //3

        stage('Build ETL Container') { //3
            steps { //4
             withEnv(["DOCKER_HOST=tcp://jenkins-docker:2376"]) { // 5
                dir('jenkins') { // 6
                // Assumant que le Dockerfile principal est nommé "Dockerfile" et se trouve dans jenkins/
                    sh 'docker build -t ${APP_IMAGE} .'
                            } // 6 - Ferme dir('jenkins')
                        } // 5

                    } // 4
                 } // 3

        stage('Run ETL and Upload to S3') { //3
            steps { //4
                script { //5
                    echo "Starting ETL Process using image: ${APP_IMAGE}"
                    
                    // On utilise le code du workspace monté en volume
                    sh "docker run --rm -v ${PWD}/data:/app/data ${APP_IMAGE} python etl_process.py"
                    
                    echo "ETL finished. Starting upload of output_data.csv to S3..."

                    // 2. Upload vers S3
                    sh '''
                        docker run --rm \\
                            --env AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
                            --env AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
                            --env AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
                            -v ${PWD}/data:/app/data \\
                            ${APP_IMAGE} python upload_s3.py ${S3_BUCKET_NAME} data/output_data.csv
                    '''
                } //5
            } //4
       
   
    
    post { //4
        success { //5
            echo '✅ Pipeline completed successfully!'
        } //5
        failure { //5
            echo '❌ Pipeline failed.'
            } //5
        } //4
        } //3
    } //2
} //1
pipeline {
    agent any

    environment {
        TEST_IMAGE = 'arxive-tests'
        APP_IMAGE = 'arxiv-app'
        AWS_ACCESS_KEY_ID = credentials('AWS_ACCESS_KEY_ID')
        AWS_SECRET_ACCESS_KEY = credentials('AWS_SECRET_ACCESS_KEY')
        AWS_DEFAULT_REGION = 'eu-north-1'
        S3_BUCKET_NAME ='bucketarxiv' 
    }

    stages {
        
        stage('Clone Repository') {
            steps {
                git branch: 'main', url: 'https://github.com/SeverineLEGUY/SEARCH_INTEREST_Arxiv.git'
            }
        }

        stage('Build Test Container') {
            steps {
                sh 'docker build -t ${TEST_IMAGE} -f tests/Dockerfile .'
            }
        }

        stage('Run Unit Tests') {
            steps {
                script {
                    sh """
                        docker run --rm \\
                            --env AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
                            --env AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
                            --env AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
                            -v ${PWD}/tests:/app/tests \\ ⬅️ Montage du volume pour le rapport
                            ${TEST_IMAGE} \\
                            /bin/bash -c "pytest --junitxml=/app/tests/junit-report.xml" ⬅️ Commande qui lance pytest
                    """
                }
            }
        }

        post {
                always {
                    // Archive et affiche les résultats des tests dans Jenkins
                    junit 'tests/junit-report.xml' 
                }
            }
        }

        stage('Build ETL Container') {
            steps {
                sh 'docker build -t ${APP_IMAGE} .'
            }
        }

        stage('Run ETL and Upload to S3') {
            steps {
                script {
                    echo "Starting ETL Process using image: ${APP_IMAGE}"
                    
                    // 1. Exécution de l'ETL
                    sh "docker run --rm -v ${PWD}/data:/app/data ${APP_IMAGE} python etl_process.py"
                    
                    echo "ETL finished. Starting upload of output_data.csv to S3..."

                    // 2. Upload vers S3
                    sh '''
                        docker run --rm \\
                            --env AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
                            --env AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
                            --env AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
                            -v ${PWD}/data:/app/data \\
                            ${APP_IMAGE} python upload_s3.py ${S3_BUCKET_NAME} data/output_data.csv
                    '''
                }
            }
        }
    }

    }

    post {
        success {
            echo '✅ Test Pipeline completed successfully! Tests results sent to your Bucket!'
        }
        failure {
            echo '❌ Test Pipeline failed.'
        }
    }
}
 
pipeline {
    agent any

    environment {
        TEST_IMAGE = 'arxive-tests'
        APP_IMAGE = 'arxiv-app'
        // Utiliser withCredentials pour la sécurité et la portée
        // ... (Les variables d'environnement sont bonnes, mais vérifiez les identifiants)
    }

    stages {
        // Le stage de clonage est supprimé car le SCM du job le gère
        
        stage('Build Test Container') {
            steps {
                // S'assure que l'agent a le code (via SCM auto-géré) et le construit
                withEnv(["DOCKER_HOST=tcp://jenkins-docker:2376"]) {
                    dir('jenkins') {
                          sh 'docker build -t arxive-tests -f Dockerfile .' 
                        }
                 }
            }
        }

        stage('Run Unit Tests') {
            steps {
                script {
                    withEnv(["DOCKER_HOST=tcp://jenkins-docker:2376"]) {
                        sh """
                            docker run --rm \\
                                --env AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
                                --env AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
                                --env AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
                                -v ${PWD}/jenkins:/app/tests \\ 
                                ${TEST_IMAGE} \\
                                /bin/bash -c "pytest --junitxml=/app/tests/junit-report.xml"
                        """
                    }
                }
            }
            post {
                always {
                    junit 'jenkins/junit-report.xml' 
                }
            }
        }

        stage('Build ETL Container') {
            steps {
                sh 'docker build -t ${APP_IMAGE} .'
            }
        }

        stage('Run ETL and Upload to S3') {
            steps {
                script {
                    echo "Starting ETL Process using image: ${APP_IMAGE}"
                    
                    // On utilise le code du workspace monté en volume
                    sh "docker run --rm -v ${PWD}/data:/app/data ${APP_IMAGE} python etl_process.py"
                    
                    echo "ETL finished. Starting upload of output_data.csv to S3..."

                    // 2. Upload vers S3
                    sh '''
                        docker run --rm \\
                            --env AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
                            --env AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
                            --env AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
                            -v ${PWD}/data:/app/data \\
                            ${APP_IMAGE} python upload_s3.py ${S3_BUCKET_NAME} data/output_data.csv
                    '''
                }
            }
        }
    } 
    
    post { 
        success {
            echo '✅ Pipeline completed successfully!'
        }
        failure {
            echo '❌ Pipeline failed.'
        }
    }
}